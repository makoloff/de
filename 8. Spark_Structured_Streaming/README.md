# Проект 8-го спринта

## Схема проекта
![Image alt](https://github.com/makoloff/de/tree/main/8.%20Spark_Structured_Streaming/img)


## Реализация приложения
### Шаг 1. Проверка работы потока.
1. Для этого с помощью kcat в одном окне терминала читаем входной топик.
2. С помощью kcat отправляем сообщение в топик, из которого читаем сообщения. Проверяем, что в терминале, где был запущен консьюмер, отображается отправленное сообщение. 
### Шаг 2. Прочитать данные об акциях из Kafka.
1. С помощью PySpark пишем код стриминга для:
 a. чтения сообщений об акциях из Kafka;
 b. вывода сообщений в консоль.
### Шаг 3. Прочитать данные о подписчиках из Postgres.
1. Дополняем код чтением статичных данных из таблицы Postgres `subscribers_restaurants` и выводом содержимого на консоль.
### Шаг 4. Преобразование JSON в датафейм. 
Сообщения из Kafka находятся в формате переменных «ключ-значение» (англ. “key-value”). В переменной value лежит содержимое JSON-файла.
1. Десериализуем value в JSON, затем JSON из value в датафрейм, где колонками будут ключи в формате JSON.
2. Фильтруем сообщения с рекламными кампаниями, для которых текущее время находится в промежутке между временем старта и временем окончания кампании.
### Шаг 5. Провести JOIN потоковых и статичных данных.
1. Джойним данные из Kafka с данными из Postgres по полю restaurant_id.
2. Добавляем колонку с датой создания — текущей датой.
### Шаг 6. Отправить результаты JOIN в Postgres для аналитики фидбэка.
Нам нужно отправлять результаты обработки сразу в два стока — Kafka для push-уведомлений и Postgres для аналитики фидбэка. В обоих случаях будем  использовать метод foreachBatch(…).
1. Отправляем данные в Postgres через функцию foreachBatch().
### Шаг 7. Отправить данные, сериализованные в формат JSON, в Kafka для push-уведомлений.
1. Создаем датафрейм для отправки в Kafka.
2. Сериализуем данные данные из датафрейма в JSON, в сообщение формата «ключ-значение» и кладем в поле value без поля feedback. 
3. через функцию foreachBatch() отправляем данные в выходной топик.
### Шаг 8. Персистентность датафрейма.
1. Сохраняем датафрейм в памяти после объединения данных.
2. После отправки данных в два стока очищаем память от датафрейма.


### Описание задачи
Ваш агрегатор для доставки еды набирает популярность и вводит новую опцию — подписку. Она открывает для пользователей ряд возможностей, одна из них которых — добавлять рестораны в избранное. Только тогда пользователю будут поступать уведомления о специальных акциях с ограниченным сроком действия. Систему, которая поможет реализовать это обновление, вам и нужно будет создать.

Благодаря обновлению рестораны смогут привлечь новую аудиторию, получить фидбэк на новые блюда и акции, продать профицит товара и увеличить продажи в непиковые часы. Акции длятся недолго, всего несколько часов, и часто бывают ситуативными, а значит, важно быстро доставить их кругу пользователей, у которых ресторан добавлен в избранное. 

Система работает так:

- Ресторан отправляет через мобильное приложение акцию с ограниченным предложением. Например, такое: «Вот и новое блюдо — его нет в обычном меню. Дарим на него скидку 70% до 14:00! Нам важен каждый комментарий о новинке».
- Сервис проверяет, у кого из пользователей ресторан находится в избранном списке.
- Сервис формирует заготовки для push-уведомлений этим пользователям о временных акциях. Уведомления будут отправляться только пока действует акция.

Сервис будет: 
1. читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени.
как выглядит входное сообщение:
`first_message:{"restaurant_id": "123e4567-e89b-12d3-a456-426614174000","adv_campaign_id": "123e4567-e89b-12d3-a456-426614174003","adv_campaign_content": "first campaign","adv_campaign_owner": "Ivanov Ivan Ivanovich","adv_campaign_owner_contact": "iiivanov@restaurant.ru","adv_campaign_datetime_start": 1659203516,"adv_campaign_datetime_end": 2659207116,"datetime_created": 1659131516}`
2. получать список подписчиков из базы данных Postgres.
3. джойнить данные из Kafka с данными из БД.
4. сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka.
5. отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане, а ещё вставлять записи в Postgres, чтобы впоследствии получить фидбэк от пользователя. Сервис push-уведомлений будет читать сообщения из Kafka и формировать готовые уведомления.
выходное сообщение должно иметь вид:
`{"restaurant_id":"123e4567-e89b-12d3-a456-426614174000","adv_campaign_id":"123e4567-e89b-12d3-a456-426614174003","adv_campaign_content":"first campaign","adv_campaign_owner":"Ivanov Ivan Ivanovich","adv_campaign_owner_contact":"iiivanov@restaurant.ru","adv_campaign_datetime_start":1659203516,"adv_campaign_datetime_end":2659207116,"client_id":"023e4567-e89b-12d3-a456-426614174000","datetime_created":1659131516,"trigger_datetime_created":1659304828}`

Добавляются два новых поля: 
- `"client_id":"023e4567-e89b-12d3-a456-426614174000"` — UUID подписчика ресторана, который достаётся из таблицы Postgres.
- `"trigger_datetime_created":1659304828` — время создания триггера, то есть выходного сообщения. Оно добавляется во время создания сообщения.
