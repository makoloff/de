# Проект 5


## Описание задачи

Чтобы привлечь новых пользователей, маркетологи хотят разместить на сторонних сайтах рекламу сообществ с высокой активностью. 
Вам нужно определить группы, в которых начала общаться большая часть их участников. В терминологии маркетинга их бы назвали пабликами с высокой конверсией в первое сообщение.


Необходимо:
Перенести из S3 в staging-слой новые данные о входе и выходе пользователей из групп — файл group_log.csv.

Лог работы групп group_log.csv содержит:
- group_id — уникальный идентификатор группы;
- user_id — уникальный идентификатор пользователя;
- user_id_from — поле для отметки о том, что пользователь не сам вступил в группу, а его добавил другой участник. Если пользователя пригласил в группу кто-то другой, поле будет непустым.
- event — действие, которое совершено пользователем user_id. Возможны следующие варианты:
- create — пользователь создал группу;
- add — пользователь user_id вступил сам или был добавлен в группу;
- leave — пользователь user_id покинул группу.
- datetime — время совершения event.

Создать в слое постоянного хранения таблицы для новых данных.Например, по схеме:
![Image alt](https://github.com/makoloff/de/blob/main/9.%20Yandex_Cloud/img/dwh_schema_cloud_infra.jpg)

Перенести новые данные из staging-области в слой DDS.
Рассчитать конверсионные показатели для десяти самых старых групп:
количество пользователей, вступивших в группу, — cnt_added_users;
количество участников группы, которые написали хотя бы одно сообщение, — cnt_users_in_group_with_messages;
конверсию в первое сообщение из вступления в группу — group_conversion.

### Инструкция по выполнению проекта
#### Шаг 1. Загрузить данные из S3
Напишите DAG в Airflow, чтобы подключиться к бакету sprint6 в S3 и выгрузить файл group_log.csv в папку data.
#### Шаг 2. Создать таблицу group_log в Vertica
Заведите таблицу group_log в схеме *_STAGING с учётом формата данных в скачанном файле.
#### Шаг 3. Загрузить данные в Vertica
Реализуйте DAG, который будет считывать из папки data файл group_log.csv и загружать его в созданную таблицу group_log в схеме *__STAGING.

#### Шаг 4. Создать таблицу связи
Добавьте в схему *__DWH таблицу связи l_user_group_activity с такими полями:
hk_l_user_group_activity — основной ключ типа INT;
hk_user_id — внешний ключ типа INT, который связан с основным ключом хаба MY__DWH.h_users;
hk_group_id — внешний ключ типа INT, который связан с основным ключом хаба MY__DWH.h_groups;
load_dt — временная отметка типа DATETIME о том, когда были загружены данные;
load_src — данные об источнике типа VARCHAR(20).

#### Шаг 5. Создать скрипты миграции в таблицу связи
Напишите SQL-скрипт миграции данных из *__STAGING.group_log в линк l_user_group_activity.
Вставьте в таблицу l_user_group_activity данные с помощью INSERT и запроса из таблиц *__STAGING.group_log . Вам потребуется сделать JOIN хэш-ключей из таблицы хабов пользователей *__DWH.h_users и таблицы групп *__DWH.h_groups.

#### Шаг 6. Создать и наполнить сателлит
Добавьте в схему *__DWH сателлит s_auth_history и наполните его данными из *__STAGING.group_log и необходимых таблиц в хранилище через скрипт миграции.
Таблицу-сателлит создайте с набором полей:
hk_l_user_group_activity — внешний ключ к ранее созданной таблице связей типа INT. Напомним, в сателлитах нет первичных ключей.
user_id_from — идентификатор того пользователя, который добавил в группу другого. Если новый участник вступил в сообщество сам, это поле пустое. Задайте атрибуту user_id_from тип INT и наполните его из исходных загруженных данных *__STAGING.group_log.
event — событие пользователя в группе;
event_dt — дата и время, когда совершилось событие;
load_dt и load_src — знакомые вам технические поля.
Для наполнения сателлита используйте основной источник загруженных данных *__STAGING.group_log в связке с необходимыми таблицами:
MY__DWH.h_groups — для связи по хеш-ключу группы данных из MY__DWH.l_user_group_activity;
MY__DWH.h_users — для связи по хэш-ключу пользователя данных из MY__DWH.l_user_group_activity

#### Шаги 7. Подготовить CTE для ответов бизнесу
Вы мигрировали данные и разложили их в реализованной модели данных для оптимального хранения. Теперь вам нужно реализовать два временные таблицы (Common Table Expression — CTE) для оптимального обращения к данным. К ним вы обратитесь, чтобы получить нужные ответы о доле пользователей, которые пишут в группах, относительно пользователей, которые просто вступили в группу.

#### Шаг 7.1. Подготовить CTE user_group_messages
Начните с таблицы, которая поможет с анализом доли пользователей, которые активны в группах (user_group_messages) — пишут туда что-то.
Напишите SQL-запрос, который создаст CTE user_group_messages, а внутри:
реализуйте вложенный запрос, который будет возвращать хэш-ключ каждой группы hk_group_id и количество пользователей в группе, которые написали хотя бы раз — cnt_users_in_group_with_messages
не забудьте, что нужно учитывать только уникальных пользователей
Запустите запрос к данным из user_group_messages для подтверждения корректности расчётов.

#### Шаг 7.2. Подготовить CTE user_group_log
Создайте временную таблицу, из которой сможете получать количество пользователей, вступивших в группы.
Напишите SQL-запрос, который создаст CTE user_group_log, а внутри:
реализуйте вложенный запрос, который вернёт хэш-ключ каждой группы hk_group_id и количество пользователей в группе, которые в неё просто вступили cnt_added_users
не забудьте учесть только уникальных пользоватей
возьмите только 10 самых ранних созданных групп (таблица MY__DWH.h_groups, самые ранние группы по значению в поле registration_dt)
для получения всех событий добавления в группу event = 'add’ удобно использовать таблицу MY__DWH.s_auth_history
Запустите запрос к данным из user_group_log для подтверждения корректности расчётов.

#### Шаг 7.3. Написать запрос и ответить на вопрос бизнеса
Временные таблицы вы написали, теперь можно обратиться к ним, чтобы вывести нужные бизнесу ответы.
Напишите SQL-запрос, который выведет по десяти самым старым группам:
Хэш-ключ группы hk_group_id
Количество новых пользователей группы (event = add). Назовите поле cnt_added_users
Количество пользователей группы, которые написали хотя бы одно сообщение. Назовите поле cnt_users_in_group_with_messages
Долю пользователей группы, которые начали общаться. Назовите выводимое поле group_conversion
Отсортируйте результаты по убыванию значений поля group_conversion.


### Описание
Репозиторий предназначен для сдачи проекта №5.

### Как работать с репозиторием
1. В вашем GitHub-аккаунте автоматически создастся репозиторий `de-project-5` после того, как вы привяжете свой GitHub-аккаунт на Платформе.
2. Скопируйте репозиторий на свой компьютер. В качестве пароля укажите ваш `Access Token`, который нужно получить на странице [Personal Access Tokens](https://github.com/settings/tokens)):
	* `git clone https://github.com/{{ username }}/de-project-5.git`
3. Перейдите в директорию с проектом: 
	* `cd de-project-5`
4. Выполните проект и сохраните получившийся код в локальном репозитории:
	* `git add .`
	* `git commit -m 'my best commit'`
5. Обновите репозиторий в вашем GutHub-аккаунте:
	* `git push origin main`

### Структура репозитория
Вложенные файлы в репозиторий будут использоваться для проверки и предоставления обратной связи по проекту. Поэтому постарайтесь публиковать ваше решение согласно установленной структуре — так будет проще соотнести задания с решениями.

Внутри `src` расположены две папки:
- `/src/dags`, папка для python кода ваших dag для airflow. Здесь необходимо размещать файлы разрешением `'.py'`;
- `/src/sql`, папка для sql кода ваших скриптов для базы данных Vertica. Здесь необходимо размещать файлы с разрешением `'.sql'`.

Для удобства проверки рекомендуем именовать файлы согласно шагам, например, для шага 1 в задании нужно создать файл с названием `'1_*.py'` и так далее.

### Как запустить контейнер
Запустите локально команду:
```
docker run \
-d \
-p 3000:3000 \
-p 3002:3002 \
-p 15432:5432 \
--mount src=airflow_sp5,target=/opt/airflow \
--mount src=lesson_sp5,target=/lessons \
--mount src=db_sp5,target=/var/lib/postgresql/data \
--name=de-sprint-5-server-local \
sindb/de-pg-cr-af:latest
```

После того как запустится контейнер, вам будут доступны:
- Airflow
	- `localhost:3000/airflow`
- БД
	- `jovyan:jovyan@localhost:15432/de`
